name: Scrape Events

on:
  schedule:
    # Run every 6 hours at off-peak times (midnight, 6 AM, noon, 6 PM UTC)
    # These times are: 3 AM, 9 AM, 3 PM, 9 PM in Bahrain (UTC+3)
    # Chosen to minimize impact on source websites during peak hours
    - cron: '0 0,6,12,18 * * *'
  workflow_dispatch:  # Manual trigger

env:
  NODE_ENV: production
  # Bot identification
  BOT_USER_AGENT: 'BahrainNightsBot/1.0 (+https://bahrainnights.com/bot-info; events aggregator)'

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 45  # Increased to account for rate limiting delays

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'

      - name: Install dependencies
        run: |
          rm -rf node_modules package-lock.json
          npm install

      - name: Install Puppeteer browsers
        run: |
          # Install Chrome for Puppeteer
          npx puppeteer browsers install chrome

      - name: Run event scrapers
        env:
          # Supabase
          NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}

          # Claude AI for content rewriting
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          CLAUDE_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}

          # AWS S3 for image uploads
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: ${{ secrets.AWS_REGION }}
          AWS_S3_BUCKET: ${{ secrets.AWS_S3_BUCKET }}
          # Also set with BAHRAINNIGHTS prefix for compatibility
          BAHRAINNIGHTS_AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          BAHRAINNIGHTS_AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          BAHRAINNIGHTS_AWS_REGION: ${{ secrets.AWS_REGION }}
          BAHRAINNIGHTS_S3_BUCKET: ${{ secrets.AWS_S3_BUCKET }}

          # Public URL for S3
          NEXT_PUBLIC_S3_URL: ${{ secrets.NEXT_PUBLIC_S3_URL }}
          NEXT_PUBLIC_CDN_URL: ${{ secrets.NEXT_PUBLIC_CDN_URL }}

          # Replicate AI for image generation (fallback when no source image)
          REPLICATE_API_TOKEN: ${{ secrets.REPLICATE_API_TOKEN }}
        run: |
          echo "Starting BahrainNights Event Scraper"
          echo "Bot User Agent: $BOT_USER_AGENT"
          echo "Run time (UTC): $(date -u)"
          echo "Run time (Bahrain): $(TZ='Asia/Bahrain' date)"
          echo ""
          npx tsx scrapers/events/index.ts

      - name: Report status
        if: always()
        run: |
          echo ""
          echo "════════════════════════════════════════════════════════════"
          echo "          Event Scraper Workflow Complete"
          echo "════════════════════════════════════════════════════════════"
          echo "Status: ${{ job.status }}"
          echo "Completed at (UTC): $(date -u)"
          echo "Completed at (Bahrain): $(TZ='Asia/Bahrain' date)"
          echo ""
          if [ "${{ job.status }}" == "failure" ]; then
            echo "⚠️ Scraper encountered errors. Check logs above for details."
          else
            echo "✅ Scraper completed successfully."
          fi
